{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Workflow with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset used with this workflow is derived from [Fannie Maeâ€™s Single-Family Loan Performance Data](http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html) with all rights reserved by Fannie Mae. This processed dataset is redistributed with permission and consent from Fannie Mae.\n",
    "\n",
    "Preprocessing ETL has already been precalculated and is located at /tmp/eoldridge/fnma_full_data_proc_out4/dnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Deep Neural Network\n",
    "\n",
    "### Model\n",
    "The model constructed below starts with an initial embedding layer ([`torch.nn.EmbeddingBag`](https://pytorch.org/docs/stable/nn.html#embeddingbag)) that takes the indices from the ETL pipeline, looks up the embeddings in the hash table and takes their mean. This vector then passes to a [multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) which finally outputs a single score.\n",
    "\n",
    "Many of the model architecture parameters can be configured by the user such as embedding dimension, number and size of hidden layers, and activation functions.\n",
    "\n",
    "### Training\n",
    "To cut down on boilerplate code and realize the benefits of [early stopping](https://en.wikipedia.org/wiki/Early_stopping)\n",
    "we use the [`ignite`](https://pytorch.org/ignite/) library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Beyond the dependencies that come installed in the standard \n",
    "[RAPIDS docker containers](https://hub.docker.com/r/rapidsai/rapidsai) we'll also\n",
    "need the following `pip` dependencies installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\n",
      "\u001b[K     |################################| 676.9MB 20kB/s \n",
      "\u001b[?25hCollecting pytorch-ignite\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/7b/1da69e5fdcb70e8f40ff3955516550207d5f5c81b428a5056510e72c60c5/pytorch_ignite-0.2.0-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K     |################################| 81kB 34.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /conda/envs/rapids/lib/python3.6/site-packages (from torch) (1.16.2)\n",
      "Installing collected packages: torch, pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.2.0 torch-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snakeviz\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/b5/2672d76c4d21debc451aaa4cc49ef5b1af1796d4627184db9f6a3b6a6401/snakeviz-2.0.0-py2.py3-none-any.whl (281kB)\n",
      "\u001b[K     |################################| 286kB 9.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tornado>=2.0 in /conda/envs/rapids/lib/python3.6/site-packages (from snakeviz) (6.0.2)\n",
      "Installing collected packages: snakeviz\n",
      "Successfully installed snakeviz-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install snakeviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE\n",
    "Most of the details are buried/organized within the .py files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.2+0.g3ebd286.dirty'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf\n",
    "cudf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETL - Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_quantiles = 20  # Used for computing histograms of continuous features\n",
    "num_features = 2 ** 22  # When hashing features range will be [0, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training - Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "hidden_dims = [600,600,600,600]\n",
    "\n",
    "device = 'cuda'\n",
    "dropout = None  # Can add dropout probability in [0, 1] here\n",
    "activation = nn.ReLU()\n",
    "\n",
    "batch_size = 80960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Dataset from Parquet\n",
    "The preprocessing ETL has already been precalculated and is stored at: /tmp/eoldridge/fnma_full_data_proc_out4/dnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1M\n",
      "drwxr-xr-x 1 10128 10004 0M May 29 18:38 .\n",
      "drwxr-xr-x 3 root  root  1M Jul  9 14:14 ..\n",
      "drwxr-xr-x 1 10128 10004 0M May 29 18:38 test\n",
      "drwxr-xr-x 1 10128 10004 0M May 29 18:38 train\n",
      "drwxr-xr-x 1 10128 10004 0M May 29 18:38 validation\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/data/mortgage/'\n",
    "!ls -al --block-size=M /data/mortgage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import run_training\n",
    "from model import MortgageNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model = MortgageNetwork(num_features, embedding_size, hidden_dims,\n",
    "                        dropout=dropout, activation=activation, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Iteration[6/107] Loss: 0.22248 Example/s: 34978.405 (Total examples: 485760)\n",
      "Epoch[1] Iteration[12/107] Loss: 0.12772 Example/s: 63785.024 (Total examples: 971520)\n",
      "Epoch[1] Iteration[18/107] Loss: 0.11654 Example/s: 88085.466 (Total examples: 1457280)\n",
      "Epoch[1] Iteration[24/107] Loss: 0.10037 Example/s: 108607.839 (Total examples: 1943040)\n",
      "Epoch[1] Iteration[30/107] Loss: 0.09003 Example/s: 126223.986 (Total examples: 2428800)\n",
      "Epoch[1] Iteration[36/107] Loss: 0.07810 Example/s: 141767.991 (Total examples: 2914560)\n",
      "Epoch[1] Iteration[42/107] Loss: 0.06543 Example/s: 155476.671 (Total examples: 3400320)\n",
      "Epoch[1] Iteration[48/107] Loss: 0.06171 Example/s: 167460.702 (Total examples: 3886080)\n",
      "Epoch[1] Iteration[54/107] Loss: 0.06012 Example/s: 178209.619 (Total examples: 4371840)\n",
      "Epoch[1] Iteration[60/107] Loss: 0.05507 Example/s: 187614.853 (Total examples: 4857600)\n",
      "Epoch[1] Iteration[66/107] Loss: 0.05189 Example/s: 196138.810 (Total examples: 5343360)\n",
      "Epoch[1] Iteration[72/107] Loss: 0.04752 Example/s: 203586.347 (Total examples: 5829120)\n",
      "Epoch[1] Iteration[78/107] Loss: 0.05184 Example/s: 210487.771 (Total examples: 6314880)\n",
      "Epoch[1] Iteration[84/107] Loss: 0.04748 Example/s: 216833.346 (Total examples: 6800640)\n",
      "Epoch[1] Iteration[90/107] Loss: 0.04862 Example/s: 222422.134 (Total examples: 7286400)\n",
      "Epoch[1] Iteration[96/107] Loss: 0.04729 Example/s: 227761.963 (Total examples: 7772160)\n",
      "Epoch[1] Iteration[102/107] Loss: 0.04640 Example/s: 232506.656 (Total examples: 8257920)\n",
      "Number of targets for PR-AUC-Curve: 9135\n",
      "Saving state to /tmp/tmpn1kpmtuy/best_state.pth.\n",
      "Validation Results - Epoch: 1\n",
      "\tPR-AUC: 0.82180\n",
      "Epoch[2] Iteration[108/107] Loss: 0.04632 Example/s: 14565.860 (Total examples: 8743680)\n",
      "Epoch[2] Iteration[114/107] Loss: 0.04672 Example/s: 82125.188 (Total examples: 9229440)\n",
      "Epoch[2] Iteration[120/107] Loss: 0.04642 Example/s: 127890.452 (Total examples: 9715200)\n",
      "Epoch[2] Iteration[126/107] Loss: 0.04491 Example/s: 160743.274 (Total examples: 10200960)\n",
      "Epoch[2] Iteration[132/107] Loss: 0.04418 Example/s: 186261.398 (Total examples: 10686720)\n",
      "Epoch[2] Iteration[138/107] Loss: 0.04654 Example/s: 205874.361 (Total examples: 11172480)\n",
      "Epoch[2] Iteration[144/107] Loss: 0.04371 Example/s: 221950.853 (Total examples: 11658240)\n",
      "Epoch[2] Iteration[150/107] Loss: 0.04690 Example/s: 234578.961 (Total examples: 12144000)\n",
      "Epoch[2] Iteration[156/107] Loss: 0.04452 Example/s: 245165.636 (Total examples: 12629760)\n",
      "Epoch[2] Iteration[162/107] Loss: 0.04249 Example/s: 254073.661 (Total examples: 13115520)\n",
      "Epoch[2] Iteration[168/107] Loss: 0.04482 Example/s: 261667.440 (Total examples: 13601280)\n",
      "Epoch[2] Iteration[174/107] Loss: 0.04406 Example/s: 268675.547 (Total examples: 14087040)\n",
      "Epoch[2] Iteration[180/107] Loss: 0.04398 Example/s: 274667.428 (Total examples: 14572800)\n",
      "Epoch[2] Iteration[186/107] Loss: 0.04239 Example/s: 279670.544 (Total examples: 15058560)\n",
      "Epoch[2] Iteration[192/107] Loss: 0.04467 Example/s: 284241.392 (Total examples: 15544320)\n",
      "Epoch[2] Iteration[198/107] Loss: 0.04480 Example/s: 288460.377 (Total examples: 16030080)\n",
      "Epoch[2] Iteration[204/107] Loss: 0.04185 Example/s: 292604.541 (Total examples: 16515840)\n",
      "Epoch[2] Iteration[210/107] Loss: 0.04350 Example/s: 296383.598 (Total examples: 17001600)\n",
      "Number of targets for PR-AUC-Curve: 9135\n",
      "Saving state to /tmp/tmpn1kpmtuy/best_state.pth.\n",
      "Validation Results - Epoch: 2\n",
      "\tPR-AUC: 0.83949\n",
      "Epoch[3] Iteration[216/107] Loss: 0.04280 Example/s: 30774.090 (Total examples: 17487360)\n",
      "Epoch[3] Iteration[222/107] Loss: 0.04357 Example/s: 98434.415 (Total examples: 17973120)\n",
      "Epoch[3] Iteration[228/107] Loss: 0.04461 Example/s: 143448.882 (Total examples: 18458880)\n",
      "Epoch[3] Iteration[234/107] Loss: 0.04089 Example/s: 175087.252 (Total examples: 18944640)\n",
      "Epoch[3] Iteration[240/107] Loss: 0.04101 Example/s: 198949.623 (Total examples: 19430400)\n",
      "Epoch[3] Iteration[246/107] Loss: 0.04325 Example/s: 217830.328 (Total examples: 19916160)\n",
      "Epoch[3] Iteration[252/107] Loss: 0.04429 Example/s: 232905.728 (Total examples: 20401920)\n",
      "Epoch[3] Iteration[258/107] Loss: 0.04299 Example/s: 244849.703 (Total examples: 20887680)\n",
      "Epoch[3] Iteration[264/107] Loss: 0.04207 Example/s: 254722.303 (Total examples: 21373440)\n",
      "Epoch[3] Iteration[270/107] Loss: 0.04117 Example/s: 264162.732 (Total examples: 21859200)\n",
      "Epoch[3] Iteration[276/107] Loss: 0.04230 Example/s: 271409.723 (Total examples: 22344960)\n",
      "Epoch[3] Iteration[282/107] Loss: 0.04404 Example/s: 277416.547 (Total examples: 22830720)\n",
      "Epoch[3] Iteration[288/107] Loss: 0.04254 Example/s: 282792.519 (Total examples: 23316480)\n",
      "Epoch[3] Iteration[294/107] Loss: 0.04055 Example/s: 287380.775 (Total examples: 23802240)\n",
      "Epoch[3] Iteration[300/107] Loss: 0.04127 Example/s: 291753.166 (Total examples: 24288000)\n",
      "Epoch[3] Iteration[306/107] Loss: 0.04203 Example/s: 295393.677 (Total examples: 24773760)\n",
      "Epoch[3] Iteration[312/107] Loss: 0.04053 Example/s: 298519.123 (Total examples: 25259520)\n",
      "Epoch[3] Iteration[318/107] Loss: 0.04166 Example/s: 301612.637 (Total examples: 25745280)\n",
      "Number of targets for PR-AUC-Curve: 9135\n",
      "Score did not improve! EarlyStopping: 1 / 4\n",
      "Loading state from /tmp/tmpn1kpmtuy/best_state.pth.\n",
      "Validation Results - Epoch: 3\n",
      "\tPR-AUC: 0.83330\n",
      "Number of targets for PR-AUC-Curve: 9048\n",
      "Saving state to /tmp/tmpn1kpmtuy/best_state.pth.\n",
      "Final Test Results - PR-AUC: 0.84660\n",
      " \n",
      "*** Profile stats marshalled to file '/tmp/tmptnfy0j6r'. \n"
     ]
    }
   ],
   "source": [
    "%snakeviz run_training(model, data_dir, batch_size=batch_size, batch_dataload=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
