{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Workflow with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset used with this workflow is derived from [Fannie Maeâ€™s Single-Family Loan Performance Data](http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html) with all rights reserved by Fannie Mae. This processed dataset is redistributed with permission and consent from Fannie Mae.\n",
    "\n",
    "Preprocessing ETL has already been precalculated and is located at /tmp/eoldridge/fnma_full_data_proc_out4/dnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Deep Neural Network\n",
    "\n",
    "### Model\n",
    "The model constructed below starts with an initial embedding layer ([`torch.nn.EmbeddingBag`](https://pytorch.org/docs/stable/nn.html#embeddingbag)) that takes the indices from the ETL pipeline, looks up the embeddings in the hash table and takes their mean. This vector then passes to a [multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) which finally outputs a single score.\n",
    "\n",
    "Many of the model architecture parameters can be configured by the user such as embedding dimension, number and size of hidden layers, and activation functions.\n",
    "\n",
    "### Training\n",
    "To cut down on boilerplate code and realize the benefits of [early stopping](https://en.wikipedia.org/wiki/Early_stopping)\n",
    "we use the [`ignite`](https://pytorch.org/ignite/) library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Beyond the dependencies that come installed in the standard \n",
    "[RAPIDS docker containers](https://hub.docker.com/r/rapidsai/rapidsai) we'll also\n",
    "need the following `pip` dependencies installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\n",
      "\u001b[K     |################################| 676.9MB 20kB/s \n",
      "\u001b[?25hCollecting pytorch-ignite\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/7b/1da69e5fdcb70e8f40ff3955516550207d5f5c81b428a5056510e72c60c5/pytorch_ignite-0.2.0-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K     |################################| 81kB 34.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /conda/envs/rapids/lib/python3.6/site-packages (from torch) (1.16.2)\n",
      "Installing collected packages: torch, pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.2.0 torch-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snakeviz\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/b5/2672d76c4d21debc451aaa4cc49ef5b1af1796d4627184db9f6a3b6a6401/snakeviz-2.0.0-py2.py3-none-any.whl (281kB)\n",
      "\u001b[K     |################################| 286kB 9.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tornado>=2.0 in /conda/envs/rapids/lib/python3.6/site-packages (from snakeviz) (6.0.2)\n",
      "Installing collected packages: snakeviz\n",
      "Successfully installed snakeviz-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install snakeviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE\n",
    "Most of the details are buried/organized within the .py files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.2+0.g3ebd286.dirty'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf\n",
    "cudf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETL - Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_quantiles = 20  # Used for computing histograms of continuous features\n",
    "num_features = 2 ** 22  # When hashing features range will be [0, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training - Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "hidden_dims = [600,600,600,600]\n",
    "\n",
    "device = 'cuda'\n",
    "dropout = None  # Can add dropout probability in [0, 1] here\n",
    "activation = nn.ReLU()\n",
    "\n",
    "batch_size = 8096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Dataset from Parquet\n",
    "The preprocessing ETL has already been precalculated and is stored at: /tmp/eoldridge/fnma_full_data_proc_out4/dnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1M\n",
      "drwxr-xr-x 1 10128 10004 0M May 29 18:38 .\n",
      "drwxr-xr-x 3 root  root  1M May 29 18:50 ..\n",
      "drwxr-xr-x 1 10128 10004 0M May 29 18:38 test\n",
      "drwxr-xr-x 1 10128 10004 0M May 29 18:38 train\n",
      "drwxr-xr-x 1 10128 10004 0M May 29 18:38 validation\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/data/mortgage/'\n",
    "!ls -al --block-size=M /data/mortgage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import run_training\n",
    "from model import MortgageNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "model = MortgageNetwork(num_features, embedding_size, hidden_dims,\n",
    "                        dropout=dropout, activation=activation, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Iteration[63/1067] Loss: 0.04694 Example/s: 122547.646 (Total examples: 510048)\n",
      "Epoch[1] Iteration[126/1067] Loss: 0.03472 Example/s: 139842.134 (Total examples: 1020096)\n",
      "Epoch[1] Iteration[189/1067] Loss: 0.04061 Example/s: 146761.253 (Total examples: 1530144)\n",
      "Epoch[1] Iteration[252/1067] Loss: 0.03693 Example/s: 150355.437 (Total examples: 2040192)\n",
      "Epoch[1] Iteration[315/1067] Loss: 0.03975 Example/s: 152617.831 (Total examples: 2550240)\n",
      "Epoch[1] Iteration[378/1067] Loss: 0.03986 Example/s: 154140.674 (Total examples: 3060288)\n",
      "Epoch[1] Iteration[441/1067] Loss: 0.04132 Example/s: 155262.977 (Total examples: 3570336)\n",
      "Epoch[1] Iteration[504/1067] Loss: 0.03956 Example/s: 155617.958 (Total examples: 4080384)\n",
      "Epoch[1] Iteration[567/1067] Loss: 0.03490 Example/s: 156353.267 (Total examples: 4590432)\n",
      "Epoch[1] Iteration[630/1067] Loss: 0.03293 Example/s: 156952.977 (Total examples: 5100480)\n",
      "Epoch[1] Iteration[693/1067] Loss: 0.03787 Example/s: 157412.830 (Total examples: 5610528)\n",
      "Epoch[1] Iteration[756/1067] Loss: 0.03532 Example/s: 157815.798 (Total examples: 6120576)\n",
      "Epoch[1] Iteration[819/1067] Loss: 0.03726 Example/s: 158168.048 (Total examples: 6630624)\n",
      "Epoch[1] Iteration[882/1067] Loss: 0.04145 Example/s: 158468.754 (Total examples: 7140672)\n",
      "Epoch[1] Iteration[945/1067] Loss: 0.03592 Example/s: 158726.899 (Total examples: 7650720)\n",
      "Epoch[1] Iteration[1008/1067] Loss: 0.03771 Example/s: 158958.527 (Total examples: 8160768)\n",
      "Number of targets for PR-AUC-Curve: 9135\n",
      "Saving state to /tmp/tmp99ot_ut6/best_state.pth.\n",
      "Validation Results - Epoch: 1\n",
      "\tPR-AUC: 0.80633\n",
      "Epoch[2] Iteration[1071/1067] Loss: 0.02944 Example/s: 34516.205 (Total examples: 8670816)\n",
      "Epoch[2] Iteration[1134/1067] Loss: 0.03379 Example/s: 133228.008 (Total examples: 9180864)\n",
      "Epoch[2] Iteration[1197/1067] Loss: 0.03183 Example/s: 146138.628 (Total examples: 9690912)\n",
      "Epoch[2] Iteration[1260/1067] Loss: 0.03705 Example/s: 151204.263 (Total examples: 10200960)\n",
      "Epoch[2] Iteration[1323/1067] Loss: 0.03067 Example/s: 153918.306 (Total examples: 10711008)\n",
      "Epoch[2] Iteration[1386/1067] Loss: 0.02962 Example/s: 155614.881 (Total examples: 11221056)\n",
      "Epoch[2] Iteration[1449/1067] Loss: 0.03403 Example/s: 156778.696 (Total examples: 11731104)\n",
      "Epoch[2] Iteration[1512/1067] Loss: 0.03628 Example/s: 157624.165 (Total examples: 12241152)\n",
      "Epoch[2] Iteration[1575/1067] Loss: 0.02723 Example/s: 158247.777 (Total examples: 12751200)\n",
      "Epoch[2] Iteration[1638/1067] Loss: 0.02982 Example/s: 158727.328 (Total examples: 13261248)\n",
      "Epoch[2] Iteration[1701/1067] Loss: 0.02534 Example/s: 159117.054 (Total examples: 13771296)\n",
      "Epoch[2] Iteration[1764/1067] Loss: 0.03205 Example/s: 159441.796 (Total examples: 14281344)\n",
      "Epoch[2] Iteration[1827/1067] Loss: 0.02836 Example/s: 159707.011 (Total examples: 14791392)\n",
      "Epoch[2] Iteration[1890/1067] Loss: 0.02940 Example/s: 159903.709 (Total examples: 15301440)\n",
      "Epoch[2] Iteration[1953/1067] Loss: 0.03022 Example/s: 159993.481 (Total examples: 15811488)\n",
      "Epoch[2] Iteration[2016/1067] Loss: 0.02569 Example/s: 160133.204 (Total examples: 16321536)\n",
      "Epoch[2] Iteration[2079/1067] Loss: 0.03009 Example/s: 160253.551 (Total examples: 16831584)\n",
      "Number of targets for PR-AUC-Curve: 9135\n",
      "Score did not improve! EarlyStopping: 1 / 4\n",
      "Loading state from /tmp/tmp99ot_ut6/best_state.pth.\n",
      "Validation Results - Epoch: 2\n",
      "\tPR-AUC: 0.77700\n",
      "Epoch[3] Iteration[2142/1067] Loss: 0.03444 Example/s: 52017.604 (Total examples: 17341632)\n",
      "Epoch[3] Iteration[2205/1067] Loss: 0.03419 Example/s: 130546.425 (Total examples: 17851680)\n",
      "Epoch[3] Iteration[2268/1067] Loss: 0.03057 Example/s: 143496.887 (Total examples: 18361728)\n",
      "Epoch[3] Iteration[2331/1067] Loss: 0.03354 Example/s: 148793.304 (Total examples: 18871776)\n",
      "Epoch[3] Iteration[2394/1067] Loss: 0.03301 Example/s: 151780.943 (Total examples: 19381824)\n",
      "Epoch[3] Iteration[2457/1067] Loss: 0.02698 Example/s: 153675.711 (Total examples: 19891872)\n",
      "Epoch[3] Iteration[2520/1067] Loss: 0.03464 Example/s: 154899.428 (Total examples: 20401920)\n",
      "Epoch[3] Iteration[2583/1067] Loss: 0.03001 Example/s: 155870.983 (Total examples: 20911968)\n",
      "Epoch[3] Iteration[2646/1067] Loss: 0.03097 Example/s: 156592.243 (Total examples: 21422016)\n",
      "Epoch[3] Iteration[2709/1067] Loss: 0.02892 Example/s: 157141.706 (Total examples: 21932064)\n",
      "Epoch[3] Iteration[2772/1067] Loss: 0.03141 Example/s: 157628.947 (Total examples: 22442112)\n",
      "Epoch[3] Iteration[2835/1067] Loss: 0.02925 Example/s: 158027.008 (Total examples: 22952160)\n",
      "Epoch[3] Iteration[2898/1067] Loss: 0.03326 Example/s: 158232.320 (Total examples: 23462208)\n",
      "Epoch[3] Iteration[2961/1067] Loss: 0.03153 Example/s: 158529.570 (Total examples: 23972256)\n",
      "Epoch[3] Iteration[3024/1067] Loss: 0.03047 Example/s: 158761.503 (Total examples: 24482304)\n",
      "Epoch[3] Iteration[3087/1067] Loss: 0.02498 Example/s: 158983.136 (Total examples: 24992352)\n",
      "Epoch[3] Iteration[3150/1067] Loss: 0.03061 Example/s: 159174.030 (Total examples: 25502400)\n",
      "Number of targets for PR-AUC-Curve: 9135\n",
      "Score did not improve! EarlyStopping: 2 / 4\n",
      "Loading state from /tmp/tmp99ot_ut6/best_state.pth.\n",
      "Validation Results - Epoch: 3\n",
      "\tPR-AUC: 0.78154\n",
      "Number of targets for PR-AUC-Curve: 9048\n",
      "Saving state to /tmp/tmp99ot_ut6/best_state.pth.\n",
      "Final Test Results - PR-AUC: 0.81231\n",
      " \n",
      "*** Profile stats marshalled to file '/tmp/tmplaoxhur1'. \n",
      "Embedding SnakeViz in the notebook...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe id='snakeviz-55ab05fc-8244-11e9-8728-0242ac110003' frameborder=0 seamless width='100%' height='1000'></iframe>\n",
       "<script>$(\"#snakeviz-55ab05fc-8244-11e9-8728-0242ac110003\").attr({src:\"http://\"+document.location.hostname+\":41300/snakeviz/%2Ftmp%2Ftmplaoxhur1\"})</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%snakeviz run_training(model, data_dir, batch_size=batch_size, batch_dataload=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
